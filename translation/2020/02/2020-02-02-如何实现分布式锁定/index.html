<!doctype html><link href=https://cdn.bootcss.com/highlight.js/9.12.0/styles/monokai.min.css rel=stylesheet><script src=https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>0xab.de</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css></head><body><nav><ul class=menu><li><a href=/post/>Post</a></li><li><a href=/translation/>Translation</a></li><li><a href=/weekly/>Weekly</a></li><li><a href=/categories/>Categories</a></li><li><a href=/tags/>Tags</a></li><li><a href=/feed.xml>Subscribe</a></li></ul><hr></nav></body></html><center><span class=title><h1>[译]
如何实现分布式锁定</h1></span></center><hr>本文于 2020年02月02日 发布于 <a href=/translation/2020/02/2020-02-02-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9A/>0xab.de</a>，转载请注明出处。<hr></div><main><blockquote><p>原文地址：<a href=https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html>https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</a></p></blockquote><p>作为<a href=http://dataintensive.net/>我的书</a>研究的一部分，我在 <a href=http://redis.io/>Redis</a> 的官网上偶然发现了一个叫做 <a href=http://redis.io/topics/distlock]>Redlock</a>的算法。这个算法声称能在 Redis 上实现容错的分布式锁（准确地说是<a href=https://www.semanticscholar.org/paper/Leases%3A-an-efficient-fault-tolerant-mechanism-for-Gray-Cheriton/8965057405c1de742346eba16f20eaca2612f576>租约</a>[1]），同时在页面上向研究分布式系统的人征求意见。这个算法本能性的在我的脑海里敲响了警钟，所以我花了些时间来思考并且写下了这些笔记。</p><p>因为<a href=https://redis.io/topics/distlock>已经有十多个 Redlock 的独立实现</a>了，并且我们不知道谁已经依赖于这个算法。所有值得把我的笔记公开分享出来。我不会深入探讨 Redis 的其他方面，因为那些已经在<a href=https://aphyr.com/tags/Redis>其他地方</a>批评过了。</p><p>在我深入 Redlock 的细节之前，我要声明我十分喜欢 Redis，并且之前已经在生产环节中成功的应用过它了。我认为它非常适合在服务器之间共享一些快速变化的数据，而且如果偶尔丢了这些数据也无关紧要。例如，维护每个 IP 地址的请求计数器（出于限制的目的）和每个用户 ID 的不同 IP 地址集(用于滥用的检测)。</p><p>然而，Redis 已经逐渐进入有强一致性和持久性特点的数据管理领域，这让我很担心，因为这不是 Redis 设计的初衷。有争议的是，分布式锁定也是这一领域之一。让我们更详细地研究一下。</p><h2 id=你用锁来干什么>你用锁来干什么</h2><p>锁的目的是用来保证在多个节点尝试做同样的工作时，只有一个能够实际完成（至少在同一时间只有一个）。这个工作可能是将一些数据写入一个共享的存储系统、完成一些计算、调用一些外部的 API 或者其他类似的工作。宏观上来看，有两个原因来解释你为什么在一个分布式应用中想要一个锁：<a href=http://research.google.com/archive/chubby.html>效率和正确性</a>[2]。为了区别出这些情况，可以往下看当锁失败的时候会发生什么：</p><ul><li><strong>效率</strong>：应用锁可以避免做不必要的重复工作（例如一些代价昂贵的计算）。如果锁失效了，并且两个节点做相同的工作，结果就是成本略有增加（最后要比其他方式多给 AWS 支付5美分）或带来小小的不便（用户可能会收到两个相同的邮件通知）。</li><li><strong>正确性</strong>：应用锁可以避免并发进程相互影响最终搞乱系统的状态。如果锁失效了，并且两个节点同时处理同一部分数据，结果可能是损坏的文件、数据丢失、永久的不一致性、给病人服用了错误剂量的药物或其他严重的问题。</li></ul><p>以上两点都是需要锁的有效情况，但是你需要十分明确你是在处理哪一个情况。</p><p>我的观点是：如果只是为了效率的目的，那么没有必要承担 Redlock 的成本和复杂性，没有必要运行5台 Redis 实例来检查是否有大多数服务器都获得了锁。你最好只启动一个 Redis 实例，或是当主节点崩溃时异步复制到从节点。</p><p>如果你使用单个 Redis 实例，在 Redis 节点断电时会失去一些锁，同时可能会出现一些问题。但是如果你使用这些锁来优化效率的话，同时节点的崩溃也不会频繁发生，那么这也没什么大不了。这个“没什么大不了”的正是 Redis 的特点。至少当你依赖于一个 Redis 实例的时候，每个查看系统的人都知道这些锁是不太严谨的，并且仅用于非关键的目的。</p><p>另一方面，具有5个副本和多数投票的 Redis 算法乍一看似乎适用于对*正确性*要求很高的情况。在接下来的几个章节中我将论证它*不*适合这个目的。文章的其余部分我们假定你的锁对保障正确性至关重要，并且两个节点持有相同的锁是一个严重的错误。</p><h2 id=使用锁来保护资源>使用锁来保护资源</h2><p>让我们把 Redlock 的具体应用放一边，先讨论一下通常一个分布式锁是如何使用的（不依赖于某个锁定算法）。记住分布式系统中的锁和多线程应用中的互斥量不一样这一点很重要。由于不同的节点和网络都能以各种方式独不同的方式发生故障，所以这是一个更复杂的野兽。</p><p>例如，假如你有一个应用，其中客户端需要更新共享存储中的文件（如 HDFS 或 S3）。一个客户端首先获得一个锁，然后读取这个文件，作出一个修改，将改动写回，最终释放这个锁。这个锁避免两个客户端同时执行读-修改-写这个循环，这将会导致丢失更新。代码可能如下：</p><pre><code class=language-scala>// THIS CODE IS BROKEN
function writeData(filename, data) {
    var lock = lockService.acquireLock(filename);
    if (!lock) {
        throw 'Failed to acquire lock';
    }

    try {
        var file = storage.readFile(filename);
        var updated = updateContents(file, data);
        storage.writeFile(filename, updated);
    } finally {
        lock.release();
    }
}
</code></pre><p>不幸的是，即使你有一个完美的锁服务，上面的代码也会被破坏。下面的图展示了是怎么破坏数据的：</p><p><img src=https://martin.kleppmann.com/2016/02/unsafe-lock.png alt="Unsafe access to a resource protected by a distributed lock"></p><p>在这个例子里，持有锁的客户端在持有锁的过程中暂停了很长的时间——如 GC。锁有超时时间（也就是租约）总是一个好主意（否则，崩溃的客户端可能会永远持有一个锁，并且永远不会释放它）。然而，如果 GC 暂停的时间持续了比租约到期的时间更久，并且客户端没有意识到他已经过期了，它会继续向后执行一些不安全的操作。</p><p>这个 Bug 不是理论上的：HBASE 经常<a href=http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage>有这个问题</a>[3,4]。通常，GC 停顿非常短，但是“stop-the-world” GC 停顿已知会停顿<a href=http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/>几分钟</a>[5]——当然已经足够等到租约过期了。即使是所谓的“并发”垃圾收集器，如 HotSpot 的 CMS，也不能完全与应用程序代码并行运行——甚至它们<a href=https://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html>需要不时地停止运行</a>[6]。</p><p>如果你为你用的语言没有长时间停顿而沾沾自喜的话，也有许多其他的原因让你的进程暂停。也许你的线程尝试还没有加载进内存的地址，那么会发生一个页错误直到页从硬盘加载进来。也许你的硬盘实际上是 EBS，所以在 Amazon 拥挤的网络上读取变量不经意间变成了同步请求。也许有许多其他进程在竞争 CPU，而你遇到了<a href=https://twitter.com/aphyr/status/682077908953792512>调度器树中一个黑节点</a>。也许有人突然向进程发送了 SIGSTOP。无论如何，你的进程会被暂停。</p><p>如果你仍不相信我关于进程暂停的观点，那么可以改成考虑文件在写入请求在到达存储服务之前会遇到网络延迟的情况。像以太网和 IP 之类的分组网络可能会任意地延迟包，而<a href="https://queue.acm.org/detail.cfm?id=2655736">这种情况确实发生了</a>[7]：在一个 <a href=https://github.com/blog/1364-downtime-last-saturday>GitHub 著名事故中</a>，包在网络中大约延迟了90秒[8]。这意味着一个应用进程可能发送一个写请求，它可能在一分钟之后租约已经过期的时候到达了存储服务。</p><p>即使在管理良好的网络中，这种情况也会发生。你根本不能对时间作出任何假设，这就是无论您使用哪种锁定服务，上述代码从根本上来说都不安全的原因。</p><h2 id=用-fencing-token-让锁安全>用 fencing token 让锁安全</h2><p>这种问题的解决方案非常简单：你需要在每次请求存储服务时携带一个 fencing token。在此情况下，一个击剑令牌只是一个每次一个客户端获得锁时递增的数字（例如被锁服务递增）。在下图中说明了这一点：</p><p><img src=https://martin.kleppmann.com/2016/02/fencing-tokens.png alt="Using fencing tokens to make resource access safe"></p><p>客户端1获得了租约并且 token 为33，但是经历了长时间的暂停租约过期了。客户端2获得了租约，且 token 为34（数字总是递增的），然后携带 token 34发送写请求到存储服务。过了一会儿，客户端1恢复并且携带 token 33发送写请求到存储服务。然而，存储服务知道已经处理了一个更新的 token（34），所以它拒绝了 token 33这个请求。</p><p>记住这需要存储服务在检查 token 中扮演一个重要的角色，并且拒绝任何旧 token 的所有写请求。只要你掌握了这个窍门就不难了。只要锁服务生成严格的单调递增的 token，那么锁就是安全的。例如，如果你使用 ZooKeeper 作为锁服务，你可以使用 zxid 或 znode 版本号作为 fencing token，这样可以获得良好的状态[3]。</p><p>但是，这就引出了 Redlock 的第一个大问题：*它没有任何生成 fencing token的功能*。该算法不会生成任何保证每次客户端获得锁时都会增加的数字。这意味着，即使算法在其他方面是完美的，使用它也不安全，因为在一个客户端暂停或其包延迟的情况下，你无法防止客户端之间的竞态条件。</p><p>在我看来，要改变 Redlock 算法来开始生成 fencing token 并不容易。它使用的唯一随机值没有提供所需的单调性。仅仅在一个 Redis 节点上保留一个计数器是不够的，因为该节点可能会宕机。在多个节点上保持计数器意味着它们将不是同步的。你可能需要一个一致的算法来生成 fencing token（如果<a href=https://twitter.com/lindsey/status/575006945213485056>增加计数器</a>很简单就好了）。</p><h2 id=利用时间来解决一致性问题>利用时间来解决一致性问题</h2><p>Redlock 不能生成 fencing token 这一事实，应该已经足以成为在正确性取决于锁的情况下不使用它的理由。但还有一些其他的问题值得讨论。</p><p>在学术文献中，这种算法最实用的系统模型是<a href=http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf>具有不可靠故障检测器</a>[9]的异步模型。简单地说，这意味着算法对时间没有任何假设:进程可能会在任意长度的时间内暂停，数据包可能会在网络中被任意延迟，时钟可能会出现任意的错误，但是算法应该做正确的事情。考虑到我们上面所讨论的，这些都是非常合理的假设。</p><p>算法使用时钟的唯一目的是产生超时，以避免在节点宕机时永远等待。但是超时不一定是准确的：仅仅因为一个请求超时了，并不意味着其他节点肯定停机了——也可能是因为网络中有很大的延迟，或者您的本地时钟出错了。当用作故障检测器时，超时只是猜测出了什么问题。(如果可以的话，分布式算法将完全不需要时钟，但这样就<a href=https://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf>不可能达成一致意见</a>[10]。获取锁就像 CAS 操作，需要<a href=https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf>达成共识</a>[11]）。</p><p>请注意，Redis <a href=https://github.com/antirez/redis/blob/edd4d555df57dc84265fdfb4ef59a4678832f6da/src/server.c#L390-L404>使用 gettimeofday</a> (而不是<a href=https://linux.die.net/man/2/clock_gettime>单调时钟</a>)来确定键的<a href=https://github.com/antirez/redis/blob/f0b168e8944af41c4161249040f01ece227cfc0c/src/db.c#L933-L959>到期时间</a>。gettimeofday 的手册<a href=https://linux.die.net/man/2/gettimeofday>明确表示</a>,它返回系统时间会出现不连续的跳跃，它可能突然向前跳了几分钟，甚至向后跳了时间（如果时钟因与 NTP 服务器相差太大而<a href=https://www.eecis.udel.edu/~mills/ntp/html/clock.html>由 NTP 步进</a>，或者时钟是由管理员手动调整的）。因此，如果系统时钟正在做一些奇怪的事情，那么很容易发生的情况是， Redis 中的 key 过期比预期的要快得多或慢得多。</p><p>对于异步模型中的算法来说，这并不是一个大问题：这些算法通常确保它们的*安全性*始终保持不变，<a href=http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf>不做任何时间假设</a>[12]。只有*活性*属性依赖于超时或其他故障检测器。简单地说，这意味着这意味着即使系统中的时间是到处都是（进程暂停、网络延迟、时钟向前和向后跳跃），算法的性能可能会下降，但是算法永远不会做出错误的决定。</p><p>然而，Redlock 不是这样的。它的安全性取决于大量的时间假设：假设所有 Redis 节点在到期前大约正确的时间长度都持有 key；与有效期限相比，网络延迟较小；而且该过程的暂停比有效期短得多。</p><h2 id=糟糕的计时打破了-redlock>糟糕的计时打破了 Redlock</h2><p>让我们看一些示例，以说明 Redlock 对时序假设的依赖。 假设系统有五个 Redis 节点（A，B，C，D和E）和两个客户端（1和2）。 如果Redis节点之一上的时钟向前跳怎么办？</p><ol><li>客户端1获得 A、B、C 节点的锁，由于网络问题，无法连接到 D、E。</li><li>节点 C 上的时钟向前跳转，导致锁过期。</li><li>客户端2在 C、D、E 节点上获取锁，由于网络问题，无法访问 A 和 B。</li><li>客户端1和客户端2现在都认为他们持有锁。</li></ol><p>如果 C 在将锁持久化到磁盘之前崩溃并立即重新启动，也会发生类似的问题。出于这个原因，Redlock 文档建议<a href=http://redis.io/topics/distlock#performance-crash-recovery-and-fsync>延迟重启崩溃节点</a>的时间，至少要达到最长锁的生存时间。但是，这种重新启动延迟再次依赖于对时间的合理准确度量，如果时钟跳变，则会失败。</p><p>好吧，也许你认为时钟跳变是不现实的，因为你对正确配置 NTP 能调正时钟非常有信心。 在这种情况下，让我们看一个过程暂停可能导致算法失败的示例：</p><ol><li>客户端1请求锁定节点 A、B、C、D、E。</li><li>当客户机1的响应处于运行状态时，客户机1进入 stop-the-world GC。</li><li>锁在所有 Redis 节点上失效。</li><li>客户端2获取节点 A、B、C、D、E 上的锁。</li><li>客户端1完成 GC，并从 Redis 节点接收到响应，表明它已成功获取了锁（进程暂停后，它们已保存在客户端1的内核网络缓冲区中）。</li><li>现在，客户端1和2都认为他们持有该锁。</li></ol><p>请注意，即使 Redis 是用 C 编写的，因此没有 GC，但这对我们没有帮助：任何客户端可能会遇到 GC 暂停的系统都存在此问题。 您只能通过阻止客户端1在客户端2获得锁之后执行该锁下的任何操作来确保安全，例如使用上述防护方法。</p><p>较长的网络延迟会产生与线程暂停相同的效果。 这可能取决于您的 TCP 用户超时时间——如果你让超时时间大大短于 Redis TTL，则可能会忽略延迟的网络数据包，但是为了确保这一点，我们必须详细研究 TCP 的实现。 此外，随着超时，我们又恢复了时间测量的准确性！</p><h2 id=redlock-的同步假设>Redlock 的同步假设</h2><p>这些示例表明，只有在假设使用*同步*系统模型（即具有以下属性的系统）的情况下，Redlock 才能正常工作：</p><ul><li>有界的网络延迟（你可以保证数据包始终在一定的最大延迟内到达）</li><li>有限的过程暂停（换句话说，严格的实时约束，通常只能在汽车安全气囊系统等中找到），和</li><li>有限的时钟错误（祈祷你不会从一个<a href=http://xenia.media.mit.edu/~nelson/research/ntp-survey99/>坏的 NTP 服务器</a>上得到你的时间）</li></ul><p>请注意，同步模型并不意味着时钟完全同步：这意味着你假设<a href=http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf>网络延迟，暂停和时钟漂移的已知固定上限</a>[12]。 Redlock 假设相对于锁的生存时间而言，延迟，暂停和漂移都很小； 如果计时问题变得与生存时间一样大，则该算法将失效。</p><p>在良好的数据中心环境中，大多数时间都将满足时序假设——这被称为<a href=http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf>部分同步系统</a>[12]。 但这足够好吗？ 一旦违反这些时序假设，Redlock 可能会违反其安全属性，例如在另一个客户端过期之前将其授予租约。 如果你依靠锁来确保正确性，那么“大多数时候”是不够的——你需要*始终*保持正确性。</p><p>有大量证据表明，对于大多数实际的系统环境而言，假设同步系统模型并不安全[7,8]。不断提醒自己 GitHub 事件的<a href=https://github.com/blog/1364-downtime-last-saturday>90秒包延迟</a>。Redlock 不太可能通过 <a href=https://aphyr.com/tags/jepsen>Jepsen</a> 测试。</p><p>另一方面，为部分同步的系统模型（或带有故障检测器的异步模型）设计的共识算法实际上有工作的机会。 Raft算法，Viewstamped Replication，Zab 和 Paxos 都属于此类。 这种算法必须放弃所有时序假设。 这实际上很难：人们很容易认为网络、进程和时钟比实际情况更可靠。但是在分布式系统的混乱现实中，你必须非常小心地处理您的假设。</p><h2 id=总结>总结</h2><p>我认为 Redlock 算法是一个错误的选择，因为它“非驴非马”：对于效率优化锁来说，它繁重且昂贵，但是对于正确性取决于锁的情况，它并不是足够安全的。</p><p>特别是，该算法对时序和系统时钟进行了危险的假设（本质上假设一个同步系统具有有限的网络延迟和有限的操作执行时间），并且如果不满足这些条件，就会违反安全性。 而且，它缺乏用于生成 fencing token 的功能（可以保护系统免受网络或暂停进程中的长时间延迟）。</p><p>如果仅在尽力而为的基础上需要锁（作为效率优化，而不是为了确保正确性），我建议坚持使用 <a href=http://redis.io/commands/set>Redis 的简单单节点锁算法</a>（有条件地 set-if-not-exists 以获得锁，原子地 delete-if-value-matches 以释放锁），并在代码中非常清楚地证明锁只是近似的，有时可能会失败。 不必理会由五个 Redis 节点组成的集群。</p><p>另一方面，如果您需要使用锁来确保正确性，请不要使用 Redlock。 相反，请使用适当的共识系统（例如<a href=https://zookeeper.apache.org/>ZooKeeper</a>），可能通过实现锁的 <a href=https://curator.apache.org/curator-recipes/index.html>Curator recipes</a> 之一（至少，使用<a href=https://www.postgresql.org/>具有合理事务保证的数据库</a>）。并且请在锁定下的所有资源访问上强制使用 fencing tokens。</p><p>正如我在开始时所说的，Redis 是一个很好的工具，如果您正确使用它。 以上所有内容都不会削弱 Redis 的目的。 <a href=http://antirez.com/>Salvatore</a>（Redis 的作者） 多年来一直致力于该项目，其成功理所应当。 但是每种工具都有局限性，了解它们并据此作出计划很重要。</p><p>如果您想了解更多信息，我将在<a href=http://dataintensive.net/>我的书的第8章和第9章</a>中更详细地说明该主题，该书现已在 O’Reilly 的“早期发行版”中提供（以上图表摘自我的书）。要学习如何使用 ZooKeeper，我推荐 <a href=http://shop.oreilly.com/product/0636920028901.do>Junqueira 和 Reed 的书</a>[3]。 为了更好地介绍分布式系统理论，我推荐 <a href=http://www.distributedprogramming.net/>Cachin，Guerraoui 和 Rodrigues 的教科书</a>[13]。</p><p>感谢 <a href=https://aphyr.com/>Kyle Kingsbury</a>，<a href=https://twitter.com/skamille>Camille Fournier</a>，<a href=https://twitter.com/fpjunqueira>Flavio Junqueira</a> 和 <a href=http://antirez.com/>Salvatore Sanfilippo</a> 审阅了本文的草稿。 当然，任何错误都是我的。</p><p>2016年2月9日更新：Redlock 的原始作者 <a href=http://antirez.com/>Salvatore</a> <a href=http://antirez.com/news/101>对本文发表了反驳</a>（另请参见<a href="https://news.ycombinator.com/item?id=11065933">HN讨论</a>）。 他提出了一些意见，但我坚持我的结论。 如果有时间，我可能会在后续帖子中进行详细说明，但请形成您自己的意见——请查阅以下参考资料，其中许多参考资料都经过了严格的学术同行评审（与我们的任何一篇博文均不同）。</p><h2 id=参考>参考</h2><p>[1] Cary G Gray and David R Cheriton: “<a href=https://pdfs.semanticscholar.org/a25e/ee836dbd2a5ae680f835309a484c9f39ae4e.pdf>Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>,” at <em>12th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1989. <a href=https://dx.doi.org/10.1145/74850.74870>doi:10.<sup>1145</sup>&frasl;<sub>74850</sub>.74870</a></p><p>[2] Mike Burrows: “<a href=http://research.google.com/archive/chubby.html>The Chubby lock service for loosely-coupled distributed systems</a>,” at <em>7th USENIX Symposium on Operating System Design and Implementation</em> (OSDI), November 2006.</p><p>[3] Flavio P Junqueira and Benjamin Reed: <a href=http://shop.oreilly.com/product/0636920028901.do><em>ZooKeeper: Distributed Process Coordination</em></a>. O’Reilly Media, November 2013. ISBN: 978-1-4493-6130-3</p><p>[4] Enis Söztutar: “<a href=http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage>HBase and HDFS: Understanding filesystem usage in HBase</a>,” at <em>HBaseCon</em>, June 2013.</p><p>[5] Todd Lipcon: “<a href=http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/>Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</a>,” blog.cloudera.com, 24 February 2011.</p><p>[6] Martin Thompson: “<a href=https://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html>Java Garbage Collection Distilled</a>,” mechanical-sympathy.blogspot.co.uk, 16 July 2013.</p><p>[7] Peter Bailis and Kyle Kingsbury: “<a href="https://queue.acm.org/detail.cfm?id=2655736">The Network is Reliable</a>,” <em>ACM Queue</em>, volume 12, number 7, July 2014. <a href=https://dx.doi.org/10.1145/2639988.2639988>doi:10.<sup>1145</sup>&frasl;<sub>2639988</sub>.2639988</a></p><p>[8] Mark Imbriaco: “<a href=https://github.com/blog/1364-downtime-last-saturday>Downtime last Saturday</a>,” github.com, 26 December 2012.</p><p>[9] Tushar Deepak Chandra and Sam Toueg: “<a href=http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf>Unreliable Failure Detectors for Reliable Distributed Systems</a>,” <em>Journal of the ACM</em>, volume 43, number 2, pages 225–267, March 1996. <a href=https://dx.doi.org/10.1145/226643.226647>doi:10.<sup>1145</sup>&frasl;<sub>226643</sub>.226647</a></p><p>[10] Michael J Fischer, Nancy Lynch, and Michael S Paterson: “<a href=https://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf>Impossibility of Distributed Consensus with One Faulty Process</a>,” <em>Journal of the ACM</em>, volume 32, number 2, pages 374–382, April 1985. <a href=https://dx.doi.org/10.1145/3149.214121>doi:10.<sup>1145</sup>&frasl;<sub>3149</sub>.214121</a></p><p>[11] Maurice P Herlihy: “<a href=https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf>Wait-Free Synchronization</a>,” <em>ACM Transactions on Programming Languages and Systems</em>, volume 13, number 1, pages 124–149, January 1991. <a href=https://dx.doi.org/10.1145/114005.102808>doi:10.<sup>1145</sup>&frasl;<sub>114005</sub>.102808</a></p><p>[12] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: “<a href=http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf>Consensus in the Presence of Partial Synchrony</a>,” <em>Journal of the ACM</em>, volume 35, number 2, pages 288–323, April 1988. <a href=https://dx.doi.org/10.1145/42282.42283>doi:10.<sup>1145</sup>&frasl;<sub>42282</sub>.42283</a></p><p>[13] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues: <a href=http://www.distributedprogramming.net/><em>Introduction to Reliable and Secure Distributed Programming</em></a>, Second Edition. Springer, February 2011. ISBN: 978-3-642-15259-7, <a href=https://dx.doi.org/10.1007/978-3-642-15260-3>doi:10.<sup>1007</sup>&frasl;<sub>978</sub>-3-642-15260-3</a></p><blockquote><p>Join the discussion about this article <a href="https://news.ycombinator.com/item?id=11059738">on Hacker News</a>.</p></blockquote><h2 id=部分评论>部分评论</h2><ul><li><p>antirez：读者注意：在文章中使用 Redlock 算法的方式存在错误：获取多数后的最后一步是检查经过的总时间是否已超过锁定 TTL，在这种情况下，客户端不认为锁是有效的。 这使得 Redlock 不受客户端&lt;-&gt;锁服务器消息延迟的影响，并且在处理被锁资源的过程中，当任何其他 GC 暂停时，都会测试锁的有效性。这也等同于使用远程锁服务器时发生的情况，如果服务器在读取前暂停，则服务器发出的“确定，你持有锁”答复仍保留在内核缓冲区中。 因此，在此文章中，假设在锁定获取阶段出现网络延迟或GC暂停是一个错误。</p></li><li><p>Martin leppmann：这是正确的，我忽略了收到消息后的其他时钟检查。 但是，我相信额外的检查不会显着改变算法的属性：</p><ul><li><p>应用程序和共享资源（受锁保护的事物）之间的较大网络延迟仍可能导致资源从不再持有锁的应用程序进程接收消息，因此仍然需要防护。</p></li><li><p>最终时钟检查和资源访问之间的 GC 暂停不会被时钟检查捕获。 正如我在文章中指出的那样：“请记住，GC可以在任何时间点暂停正在运行的线程，包括那个给你带来最大问题的时间点。</p></li><li><p>所有对时钟测量精度的依赖仍然成立。</p></li></ul></li><li><p>antirez：你好 Martin，谢谢你的答复。应用程序和共享资源之间的网络延迟，以及检查*之后*但在进行实际工作之前的GC暂停在概念上都与参数的“点1”完全相同，即 GC 暂停（或其他暂停）使算法需要增量 token。因此，关于算法本身的安全性，剩下的唯一事情就是对时钟漂移的依赖性，这可以根据观点来讨论。因此恕我直言，当前版本的文章通过展示错误的算法的实，而不是引用 GC 暂停的等效处理共享资源，与 GC 暂停立即获得 token 之后并不是一个相同的场景。</p></li></ul></main><div id=disqus_thread></div><script>(function(){var d=document,s=d.createElement('script');s.src='https://arjenzhou.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><footer><hr><a href=https://arjenzhou.github.io>Home</a> | <a href=https://github.com/arjenzhou>Github</a> | <a href=mailto:zhouyang.zy@outlook.com>Email</a></footer></body></html>